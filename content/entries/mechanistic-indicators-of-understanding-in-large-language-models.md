---
title: "Mechanistic Indicators of Understanding in Large Language Models"
year: "forthcoming"
tags:
  - "ai-safety"
  - "computational-cognition"
  - "cognition"
  - "emergent-behavior"
  - "explainable-ai"
  - "feature-geometry"
  - "interpretability"
  - "latent-space"
  - "machine-understanding"
  - "llm"
  - "mechanistic-interpretability"
  - "philosophy-of-ai"
  - "representation-learning"
  - "theoretical-philosophy"
  - "understanding"
  - "conceptual-change"
categories:
  - "theoretical philosophy"
---

*arXiv*. With Pierre Beckmann. [doi:10.48550/arXiv.2507.08017](https://doi.org/10.48550/arXiv.2507.08017)

Draws on detailed technical evidence from research on mechanistic interpretability (MI) to argue that while LLMs differ profoundly from human cognition, they do more than tally up word co-occurrences: they form internal structures that are fruitfully compared to different forms of human understanding, such as conceptual, factual, and principled understanding. We synthesize MI’s most relevant findings to date while embedding them within an integrative theoretical framework for thinking about understanding in LLMs. As the phenomenon of “parallel mechanisms” shows, however, the differences between LLMs and human cognition are as philosophically fruitful to consider as the similarities.

<span><i class="fa-solid fa-tags"></i> <a href="/tags/explainable-ai/">explainable AI</a>, <a href="/tags/llm/">LLM</a>, <a href="/tags/mechanistic-interpretability/">mechanistic interpretability</a>, <a href="/tags/philosophy-of-ai/">philosophy of AI</a>, <a href="/tags/understanding/">understanding</a>, <a href="/tags/conceptual-change/">conceptual change</a></span>

<a class="download-link" href="https://philpapers.org/archive/BECMIO.pdf" aria-label="Download PDF of Mechanistic Indicators of Understanding in Large Language Models" target="_blank" rel="noopener noreferrer">
  <i class="fa-solid fa-download"></i> Download PDF
</a>
